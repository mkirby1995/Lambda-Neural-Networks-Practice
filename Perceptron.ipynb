{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Perceptron\n",
    "\n",
    "The first and simplest kind of neural network that we could talk about is the perceptron. A perceptron is just a single node or neuron of a neural network with nothing else. It can take any number of inputs and spit out an output. What a neuron does is it takes each of the input values, multplies each of them by a weight, sums all of these products up, and then passes the sum through what is called an \"activation function\" the result of which is the final value.\n",
    "\n",
    "If we were to write what is happening in some verbose mathematical notation, it might look something like this:\n",
    "\n",
    "$$\\begin{align}\n",
    " y = sigmoid(\\sum(weight_{1}input_{1} + weight_{2}input_{2} + weight_{3}input_{3}) + bias)\n",
    "\\end{align}$$\n",
    "\n",
    "\n",
    "Understanding what happens with a single neuron is important because this is the same pattern that will take place for all of our networks.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Breakdown of Perceptron Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First lets initalize some data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df.iloc[0:100, 4].values\n",
    "\n",
    "y = np.where(y == 'Iris-setosa', -1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df.iloc[0:100, [0, 2]].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x[0], y[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### After that lets's create a couple of variables representing the number of iterations and the learning rate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rate = 0.001\n",
    "niter = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now lets create some weights, we want a 1D array of zeros with a length of 1 + the number of features we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = x.shape[1]\n",
    "\n",
    "weights = np.zeros(1 + n_features)\n",
    "\n",
    "weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's create an empty list to hold our errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here is where our learning rate and iterations will come into play. I'm including alot of print statements so we can see what is happening (Verbosity = 11)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(niter):\n",
    "    err = 0\n",
    "    print('Iteration:', i, '\\n')\n",
    "    for xi, target in zip(x, y):\n",
    "        print('Inputs')\n",
    "        print('Row {} feature values:'.format(i), xi, '\\nRow {} target class:'.format(i), target, '\\n')\n",
    "        print('Initial Operations')\n",
    "        print('Dot product of feature values and weights:', np.dot(xi, weights[1:]))\n",
    "        net_input = np.dot(xi, weights[1:]) + weights[0]\n",
    "        print('Sum of dot product and our bias (aka net input):', net_input, '\\n')\n",
    "        # See np.where documentation https://docs.scipy.org/doc/numpy/reference/generated/numpy.where.html\n",
    "        prediction = np.where(net_input >= 0.0, 1, -1)\n",
    "        print('Prediction:', prediction, '\\n')\n",
    "        print('Update Weights and Bias')\n",
    "        delta_w = rate * (target - prediction)\n",
    "        print('Current weights:', weights[1:])\n",
    "        print('Weight change:', delta_w)\n",
    "        weights[1:] += delta_w * xi\n",
    "        print('Updated weights:', weights[1:])\n",
    "        print('Current bias term:', weights[0])\n",
    "        weights[0] += delta_w\n",
    "        print('Updated bias term:', weights[0])\n",
    "        err += int(delta_w != 0.0)\n",
    "        print('--------------------\\n')\n",
    "    print('************************************************\\n\\n')\n",
    "    errors.append(err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### After our model has been trained we can see its preformance history using our erors list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(1, len(errors) + 1), errors, marker='o')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Number of misclassifications')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
