{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backpropagation (Non-GD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Backpropagation is short for \"Backwards Propagation of errors\" and refers to a specific (rather calculus intensive) algorithm for how weights in a neural network are updated in reverse order at the end of each training epoch.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df.iloc[0:100, 4].values\n",
    "y = np.where(y == 'Iris-setosa', -1, 1).reshape(100, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df.iloc[0:100, [0, 2]].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "x = scaler.fit_transform(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((100, 2), (100, 1))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Breakdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2, 1000), (1000, 1))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = x.shape[1]\n",
    "hiddenNodes = 1000\n",
    "outputNodes = 1\n",
    "niter = 5\n",
    "\n",
    "weights1 = np.random.randn(inputs, hiddenNodes)\n",
    "weights2 = np.random.rand(hiddenNodes, outputNodes)\n",
    "\n",
    "weights1.shape, weights2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(s):\n",
    "    return 1 / (1+np.exp(-s))\n",
    "\n",
    "def sigmoidPrime(s):\n",
    "    return s * (1 - s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0 \n",
      "\n",
      "Initial Values\n",
      "Weights1: (2, 1000)\n",
      "Weights2: (1000, 1)\n",
      "----------------------- \n",
      "\n",
      "Feed forward \n",
      "\n",
      "Hidden sum: Dot product of weights1 and our input matrix (100, 1000) \n",
      "\n",
      "Activations of hidden sum: Values of our hidden sum that have been passed            \n",
      "  through our activation function (100, 1000) \n",
      "\n",
      "Output sum: Dot product of our activations of hidden sum and weights2 (100, 1) \n",
      "\n",
      "Activated output: Value of our output sum after pasing it though the activation function (100, 1)\n",
      "----------------------- \n",
      "\n",
      "\n",
      "Backpropagation of errors \n",
      "\n",
      "Output error: Difference between our expected and actual outputs (100, 1) \n",
      "\n",
      "Output change: Product of our output error and the actual output after it has been passed through the            derivative of our activation function (100, 1) \n",
      "\n",
      "Z2 error: Dot product of our output change and the transpose of weights2 (100, 1000) \n",
      "\n",
      "Z2 change: Product of our Z2 error and the activations of hidden sum after they have been passed           through the derivative of our activation function (100, 1000) \n",
      "\n",
      "New weights1: Dot product of the transpose of our input matrix and Z2 change (2, 1000) \n",
      "\n",
      "New weights 2: Dot product between the transpose of our activations of hidden sum and the output           change (1000, 1)\n",
      "-----------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration: 1 \n",
      "\n",
      "Initial Values\n",
      "Weights1: (2, 1000)\n",
      "Weights2: (1000, 1)\n",
      "----------------------- \n",
      "\n",
      "Feed forward \n",
      "\n",
      "Hidden sum: Dot product of weights1 and our input matrix (100, 1000) \n",
      "\n",
      "Activations of hidden sum: Values of our hidden sum that have been passed            \n",
      "  through our activation function (100, 1000) \n",
      "\n",
      "Output sum: Dot product of our activations of hidden sum and weights2 (100, 1) \n",
      "\n",
      "Activated output: Value of our output sum after pasing it though the activation function (100, 1)\n",
      "----------------------- \n",
      "\n",
      "\n",
      "Backpropagation of errors \n",
      "\n",
      "Output error: Difference between our expected and actual outputs (100, 1) \n",
      "\n",
      "Output change: Product of our output error and the actual output after it has been passed through the            derivative of our activation function (100, 1) \n",
      "\n",
      "Z2 error: Dot product of our output change and the transpose of weights2 (100, 1000) \n",
      "\n",
      "Z2 change: Product of our Z2 error and the activations of hidden sum after they have been passed           through the derivative of our activation function (100, 1000) \n",
      "\n",
      "New weights1: Dot product of the transpose of our input matrix and Z2 change (2, 1000) \n",
      "\n",
      "New weights 2: Dot product between the transpose of our activations of hidden sum and the output           change (1000, 1)\n",
      "-----------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration: 2 \n",
      "\n",
      "Initial Values\n",
      "Weights1: (2, 1000)\n",
      "Weights2: (1000, 1)\n",
      "----------------------- \n",
      "\n",
      "Feed forward \n",
      "\n",
      "Hidden sum: Dot product of weights1 and our input matrix (100, 1000) \n",
      "\n",
      "Activations of hidden sum: Values of our hidden sum that have been passed            \n",
      "  through our activation function (100, 1000) \n",
      "\n",
      "Output sum: Dot product of our activations of hidden sum and weights2 (100, 1) \n",
      "\n",
      "Activated output: Value of our output sum after pasing it though the activation function (100, 1)\n",
      "----------------------- \n",
      "\n",
      "\n",
      "Backpropagation of errors \n",
      "\n",
      "Output error: Difference between our expected and actual outputs (100, 1) \n",
      "\n",
      "Output change: Product of our output error and the actual output after it has been passed through the            derivative of our activation function (100, 1) \n",
      "\n",
      "Z2 error: Dot product of our output change and the transpose of weights2 (100, 1000) \n",
      "\n",
      "Z2 change: Product of our Z2 error and the activations of hidden sum after they have been passed           through the derivative of our activation function (100, 1000) \n",
      "\n",
      "New weights1: Dot product of the transpose of our input matrix and Z2 change (2, 1000) \n",
      "\n",
      "New weights 2: Dot product between the transpose of our activations of hidden sum and the output           change (1000, 1)\n",
      "-----------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration: 3 \n",
      "\n",
      "Initial Values\n",
      "Weights1: (2, 1000)\n",
      "Weights2: (1000, 1)\n",
      "----------------------- \n",
      "\n",
      "Feed forward \n",
      "\n",
      "Hidden sum: Dot product of weights1 and our input matrix (100, 1000) \n",
      "\n",
      "Activations of hidden sum: Values of our hidden sum that have been passed            \n",
      "  through our activation function (100, 1000) \n",
      "\n",
      "Output sum: Dot product of our activations of hidden sum and weights2 (100, 1) \n",
      "\n",
      "Activated output: Value of our output sum after pasing it though the activation function (100, 1)\n",
      "----------------------- \n",
      "\n",
      "\n",
      "Backpropagation of errors \n",
      "\n",
      "Output error: Difference between our expected and actual outputs (100, 1) \n",
      "\n",
      "Output change: Product of our output error and the actual output after it has been passed through the            derivative of our activation function (100, 1) \n",
      "\n",
      "Z2 error: Dot product of our output change and the transpose of weights2 (100, 1000) \n",
      "\n",
      "Z2 change: Product of our Z2 error and the activations of hidden sum after they have been passed           through the derivative of our activation function (100, 1000) \n",
      "\n",
      "New weights1: Dot product of the transpose of our input matrix and Z2 change (2, 1000) \n",
      "\n",
      "New weights 2: Dot product between the transpose of our activations of hidden sum and the output           change (1000, 1)\n",
      "-----------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration: 4 \n",
      "\n",
      "Initial Values\n",
      "Weights1: (2, 1000)\n",
      "Weights2: (1000, 1)\n",
      "----------------------- \n",
      "\n",
      "Feed forward \n",
      "\n",
      "Hidden sum: Dot product of weights1 and our input matrix (100, 1000) \n",
      "\n",
      "Activations of hidden sum: Values of our hidden sum that have been passed            \n",
      "  through our activation function (100, 1000) \n",
      "\n",
      "Output sum: Dot product of our activations of hidden sum and weights2 (100, 1) \n",
      "\n",
      "Activated output: Value of our output sum after pasing it though the activation function (100, 1)\n",
      "----------------------- \n",
      "\n",
      "\n",
      "Backpropagation of errors \n",
      "\n",
      "Output error: Difference between our expected and actual outputs (100, 1) \n",
      "\n",
      "Output change: Product of our output error and the actual output after it has been passed through the            derivative of our activation function (100, 1) \n",
      "\n",
      "Z2 error: Dot product of our output change and the transpose of weights2 (100, 1000) \n",
      "\n",
      "Z2 change: Product of our Z2 error and the activations of hidden sum after they have been passed           through the derivative of our activation function (100, 1000) \n",
      "\n",
      "New weights1: Dot product of the transpose of our input matrix and Z2 change (2, 1000) \n",
      "\n",
      "New weights 2: Dot product between the transpose of our activations of hidden sum and the output           change (1000, 1)\n",
      "-----------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "MSE: 2.0\n"
     ]
    }
   ],
   "source": [
    "for i in range(niter):\n",
    "    print(\"Iteration:\", i, '\\n')\n",
    "    print(\"Initial Values\")\n",
    "    print(\"Weights1:\", weights1.shape)\n",
    "    print(\"Weights2:\", weights2.shape)\n",
    "    print('-----------------------', '\\n')\n",
    "    \n",
    "    print('Feed forward', '\\n')\n",
    "    # Weighted sum of inputs and hidden layer\n",
    "    hidden_sum = np.dot(x, weights1)\n",
    "    print(\"Hidden sum: Dot product of weights1 and our input matrix\", hidden_sum.shape, '\\n')\n",
    "    \n",
    "    # Acivations of weighted sum\n",
    "    activated_hidden = sigmoid(hidden_sum)\n",
    "    print(\"Activations of hidden sum: Values of our hidden sum that have been passed \\\n",
    "           \\n  through our activation function\", activated_hidden.shape, '\\n')\n",
    "    \n",
    "    # Weight sum between hidden and output\n",
    "    output_sum = np.dot(activated_hidden, weights2)\n",
    "    print(\"Output sum: Dot product of our activations of hidden sum and weights2\", output_sum.shape, '\\n')\n",
    "    \n",
    "    # Final activation of output\n",
    "    activated_output = sigmoid(output_sum)\n",
    "    print(\"Activated output: Value of our output sum after pasing it though the activation function\", activated_output.shape)\n",
    "    print('-----------------------', '\\n\\n')\n",
    "    \n",
    "    #backprop\n",
    "    print('Backpropagation of errors', '\\n')\n",
    "    o_error = np.subtract(y, activated_output) #error\n",
    "    print(\"Output error: Difference between our expected and actual outputs\", o_error.shape, '\\n')\n",
    "    o_delta = o_error * sigmoidPrime(activated_output)\n",
    "    print(\"Output change: Product of our output error and the actual output after it has been passed through the \\\n",
    "           derivative of our activation function\", o_delta.shape, '\\n')\n",
    "    \n",
    "    z2_error = o_delta.dot(weights2.T) \n",
    "    print(\"Z2 error: Dot product of our output change and the transpose of weights2\", z2_error.shape, '\\n')\n",
    "    z2_delta = z2_error*sigmoidPrime(activated_hidden)\n",
    "    print(\"Z2 change: Product of our Z2 error and the activations of hidden sum after they have been passed\\\n",
    "           through the derivative of our activation function\", z2_delta.shape, '\\n')\n",
    "\n",
    "\n",
    "    weights1 += x.T.dot(z2_delta) \n",
    "    print(\"New weights1: Dot product of the transpose of our input matrix and Z2 change\", weights1.shape, '\\n')\n",
    "    weights2 += activated_hidden.T.dot(o_delta)    \n",
    "    print(\"New weights 2: Dot product between the transpose of our activations of hidden sum and the output\\\n",
    "           change\", weights2.shape)\n",
    "    print('-----------------------')\n",
    "    print('\\n\\n\\n')\n",
    "    \n",
    "    \n",
    "print(\"MSE:\", str(np.mean(np.square(y - activated_output))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork: \n",
    "    def __init__(self, inputs, hidden_nodes, output_nodes):\n",
    "        # Set up Archietecture \n",
    "        self.inputs = inputs\n",
    "        self.hiddenNodes = hidden_nodes\n",
    "        self.outputNodes = output_nodes\n",
    "        \n",
    "        # Initial weights\n",
    "        self.weights1 = np.random.randn(self.inputs, self.hiddenNodes)\n",
    "        self.weights2 = np.random.rand(self.hiddenNodes, self.outputNodes)\n",
    "    \n",
    "    \n",
    "    def sigmoid(self, s):\n",
    "        return 1 / (1+np.exp(-s))\n",
    "    \n",
    "    \n",
    "    def sigmoidPrime(self, s):\n",
    "        return s * (1 - s)\n",
    "    \n",
    "    \n",
    "    def feed_forward(self, X):\n",
    "        # Weighted sum of inputs and hidden layer\n",
    "        self.hidden_sum = np.dot(X, self.weights1)\n",
    "        \n",
    "        # Acivations of weighted sum\n",
    "        self.activated_hidden = self.sigmoid(self.hidden_sum)\n",
    "        \n",
    "        # Weight sum between hidden and output\n",
    "        self.output_sum = np.dot(self.activated_hidden, self.weights2)\n",
    "        \n",
    "        # Final activation of output\n",
    "        self.activated_output = self.sigmoid(self.output_sum)\n",
    "        \n",
    "        return self.activated_output\n",
    "    \n",
    "    \n",
    "    def backward(self, X, y, o):\n",
    "        # np.subtract to keep shape consistant\n",
    "        self.o_error = np.subtract(y, o) #error\n",
    "        self.o_delta = self.o_error * self.sigmoidPrime(o)\n",
    "        \n",
    "        self.z2_error = self.o_delta.dot(self.weights2.T) \n",
    "        self.z2_delta = self.z2_error*self.sigmoidPrime(self.activated_hidden)\n",
    "        \n",
    "        self.weights1 += X.T.dot(self.z2_delta) \n",
    "        self.weights2 += self.activated_hidden.T.dot(self.o_delta) \n",
    "        \n",
    "        \n",
    "    def train(self, X, y, epochs = 10000):\n",
    "        for i in range(epochs):\n",
    "            o = self.feed_forward(X)\n",
    "            self.backward(X, y, o)\n",
    "        predictions = np.where(activated_output >= 0.0, 1, -1)\n",
    "        print(\"That shizz is trained. \\n Loss =\", str(np.mean(np.square(y - predictions))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That shizz is trained. \n",
      " Loss = 2.0\n"
     ]
    }
   ],
   "source": [
    "model = NeuralNetwork(inputs = x.shape[1], hidden_nodes = 300, output_nodes = 1)\n",
    "\n",
    "model.train(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
